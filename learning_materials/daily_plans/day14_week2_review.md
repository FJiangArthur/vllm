# Day 14: Week 2 Review & CUDA Kernels Consolidation

> **Goal**: Review and consolidate all Week 2 concepts, test understanding, prepare for Week 3
> **Time**: 6-8 hours
> **Prerequisites**: Completed Days 8-13
> **Deliverables**: Completed quiz, integration project, Week 2 summary document, readiness for Week 3

---

## ğŸ“… Daily Schedule

### Morning Session (3-4 hours): Comprehensive Review

**9:00-9:30** - Week 2 Overview & Key Concepts
**9:30-10:30** - Concept Connections & Integration
**10:30-11:00** - Break + Mind Mapping
**11:00-12:30** - Comprehensive Quiz (Part 1)

### Afternoon Session (3-4 hours): Practice & Integration

**14:00-15:00** - Comprehensive Quiz (Part 2)
**15:00-16:30** - Integration Project: Optimized Inference Pipeline
**16:30-17:00** - Break
**17:00-18:00** - Week 3 Preview & Planning

### Evening (Optional, 1-2 hours): Reflection

**19:00-21:00** - Document learnings, identify gaps, prepare questions, organize notes

---

## ğŸ¯ Learning Objectives

By end of day, you should be able to:
- [ ] Explain all major Week 2 concepts clearly
- [ ] Connect CUDA optimizations across different kernels
- [ ] Implement optimized kernels from scratch
- [ ] Analyze performance bottlenecks
- [ ] Choose appropriate optimization techniques for different scenarios
- [ ] Be ready to dive into system components in Week 3

---

## ğŸ“š Morning: Comprehensive Review (9:00-12:30)

### Task 1: Week 2 Key Concepts Summary (30 min)

**Day 8: Attention Kernels Intro**
```
âœ… CUDA Memory Hierarchy:
   - Registers (fastest, ~1 cycle)
   - Shared memory (~20 cycles, 48-164 KB/SM)
   - L1/L2 Cache (automatic)
   - Global memory (slowest, ~400 cycles)

âœ… PagedAttention V1 Kernel:
   - One block per (sequence, head)
   - Load Q into shared memory
   - Loop over KV cache blocks
   - Two-pass algorithm: scores â†’ softmax â†’ output

âœ… Memory Access Patterns:
   - Coalesced vs uncoalesced access
   - Impact of block table indirection
   - Vectorized loads for better bandwidth
```

**Day 9: Advanced Attention Kernels**
```
âœ… PagedAttention V2:
   - Partitioning for long contexts
   - Parallel processing of chunks
   - Reduction to combine partials
   - 1.5-2x speedup for long sequences

âœ… Flash Attention:
   - Online softmax algorithm
   - Tiling to avoid materializing attention matrix
   - Memory: O(N) instead of O(NÂ²)
   - 2-4x speedup for long sequences

âœ… GQA/MQA:
   - Grouped-Query Attention (fewer KV heads)
   - 4x memory reduction (32 Q heads, 8 KV heads)
   - Minimal accuracy loss
   - Head mapping in kernels
```

**Day 10: Memory Management Kernels**
```
âœ… Cache Operations:
   - reshape_and_cache: Transform to paged layout
   - copy_blocks: Within GPU memory
   - swap_blocks: GPU â†” CPU transfers

âœ… Optimization Techniques:
   - Vectorized memory access (128-bit loads)
   - Coalesced access patterns
   - Pinned memory for faster transfers
   - Async operations with streams

âœ… Performance:
   - Target: 80-90% of peak bandwidth
   - A100: ~1500-1650 GB/s achievable
   - Profiling with Nsight Compute
```

**Day 11: Advanced KV Cache**
```
âœ… Prefix Caching:
   - Reuse KV blocks for common prefixes
   - Reference counting for sharing
   - 4-8x speedup for repeated prompts
   - Cache hit optimization

âœ… Kernel Specialization:
   - Head size (64, 128, 256)
   - Data type (FP16, BF16, FP8)
   - Block size (16, 32, 64 tokens)
   - 1.3-1.8x speedup from specialization

âœ… Production Integration:
   - Cache lifecycle in request flow
   - Write-back and consistency
   - Multi-stream synchronization
```

**Day 12: Quantization Kernels**
```
âœ… Quantization Methods:
   - Symmetric vs asymmetric
   - Per-tensor vs per-channel
   - INT8: 2x memory, 2-4x speed
   - INT4: 4x memory, 4-8x speed

âœ… Advanced Techniques:
   - GPTQ: Layer-wise optimal quantization
   - AWQ: Activation-aware weight quantization
   - SmoothQuant: Transfer difficulty to weights

âœ… Implementation:
   - INT4 packing (2 values per byte)
   - Fused dequantization + GEMM
   - INT8 KV cache quantization
   - FP8 for modern GPUs (H100)
```

**Day 13: Custom Operators**
```
âœ… PyTorch Extensions:
   - Setup.py with CUDAExtension
   - Pybind11 for Python bindings
   - Autograd integration

âœ… Kernel Fusion:
   - ReLU + Bias, RMSNorm + Quantize
   - Reduce kernel launches
   - Save memory bandwidth
   - 1.5-3x speedup typical

âœ… Triton:
   - Python-based GPU programming
   - Automatic optimization
   - Faster development iteration
   - Trade-offs vs raw CUDA
```

### Task 2: Concept Integration (60 min)

**ğŸ”— How CUDA Concepts Connect**:

```
Memory Hierarchy â†’ Attention Kernels:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Query loaded into shared memory         â”‚
  â”‚ â†’ Reduces global memory accesses by 20x â”‚
  â”‚ â†’ All threads in block share Q          â”‚
  â”‚ â†’ Critical for performance              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Flash Attention â†’ Memory Management:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Tiling strategy minimizes memory        â”‚
  â”‚ â†’ Never materialize full NÃ—N matrix     â”‚
  â”‚ â†’ Similar to how paged attention works  â”‚
  â”‚ â†’ Both avoid large intermediate tensors â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Quantization â†’ Kernel Fusion:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Dequantize on-the-fly during GEMM      â”‚
  â”‚ â†’ Fuses dequant + matmul operations     â”‚
  â”‚ â†’ Weights stay INT4/INT8 in memory      â”‚
  â”‚ â†’ Saves bandwidth, improves performance â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Prefix Caching â†’ Cache Kernels:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Shared blocks need careful management   â”‚
  â”‚ â†’ Reference counting in block manager   â”‚
  â”‚ â†’ Cache kernels check if block shared   â”‚
  â”‚ â†’ Skip writes to shared (read-only)     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ğŸ¯ End-to-End Optimization Stack**:

```
User Request: "Explain quantum computing"
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Python Layer (vllm/entrypoints/)             â”‚
â”‚ - Tokenize input                             â”‚
â”‚ - Create sequence, allocate blocks           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scheduler (vllm/core/)                       â”‚
â”‚ - Check prefix cache (Day 11)                â”‚
â”‚ - Reuse blocks if prefix matches             â”‚
â”‚ - Allocate only for unique suffix            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Executor (vllm/model_executor/)        â”‚
â”‚ - Load quantized weights (Day 12)            â”‚
â”‚   â†’ INT4 GPTQ/AWQ weights                    â”‚
â”‚ - Prepare inputs for GPU                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Attention Layer (GPU kernels)                â”‚
â”‚ 1. Compute Q, K, V projections               â”‚
â”‚ 2. reshape_and_cache_kernel (Day 10)         â”‚
â”‚    â†’ Write K, V to paged cache               â”‚
â”‚    â†’ Vectorized writes (128-bit)             â”‚
â”‚ 3. paged_attention_v2_kernel (Day 9)         â”‚
â”‚    â†’ Load Q into shared memory               â”‚
â”‚    â†’ Process KV in partitions                â”‚
â”‚    â†’ Fused attention + softmax               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output Layer (GPU kernels)                   â”‚
â”‚ - Fused matmul + activation (Day 13)         â”‚
â”‚ - Custom sampler kernel                      â”‚
â”‚ - Generate next token                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
Result: Fast, memory-efficient inference!

Optimizations applied:
  âœ“ Prefix caching (4x speedup)
  âœ“ INT4 quantization (4x memory reduction)
  âœ“ Flash Attention V2 (2x speedup)
  âœ“ Kernel fusion (1.5x speedup)
  âœ“ Specialized kernels (1.5x speedup)

Total: ~24x improvement over naive implementation!
```

### Task 3: Comprehensive Quiz - Part 1 (60 min)

**Section A: Memory & Performance**

**Q1**: Explain the CUDA memory hierarchy and typical access latencies.
<details>
<summary>Answer</summary>
From fastest to slowest:
- Registers: ~1 cycle, 256 KB/SM, per-thread
- Shared memory: ~20 cycles, 48-164 KB/SM, per-block
- L1 cache: ~28 cycles, 128 KB/SM, automatic
- L2 cache: ~200 cycles, 40-80 MB, automatic
- Global memory: ~400 cycles, 40-80 GB, device-wide

Key insight: Shared memory is ~20x faster than global memory, which is why attention kernels load Q into shared memory.
</details>

**Q2**: What is memory coalescing and why is it important?
<details>
<summary>Answer</summary>
Memory coalescing occurs when consecutive threads in a warp access consecutive memory addresses, allowing the GPU to combine multiple accesses into fewer memory transactions. Uncoalesced access can reduce bandwidth utilization by 10-30x. Critical for achieving peak memory throughput.
</details>

**Q3**: Calculate KV cache memory for Llama-2-7B at different precision:
- Config: 32 layers, 32 heads, head_dim=128, 2048 context, batch=64
- Compare: FP16, INT8, INT4

<details>
<summary>Answer</summary>
```
Per-token KV cache = 2 (K+V) Ã— layers Ã— heads Ã— head_dim Ã— bytes
FP16: 2 Ã— 32 Ã— 32 Ã— 128 Ã— 2 = 524,288 bytes = 512 KB/token
INT8: 2 Ã— 32 Ã— 32 Ã— 128 Ã— 1 = 262,144 bytes = 256 KB/token
INT4: 2 Ã— 32 Ã— 32 Ã— 128 Ã— 0.5 = 131,072 bytes = 128 KB/token

For batch=64, context=2048:
FP16: 64 Ã— 2048 Ã— 512 KB = 64 GB
INT8: 64 Ã— 2048 Ã— 256 KB = 32 GB (2x reduction)
INT4: 64 Ã— 2048 Ã— 128 KB = 16 GB (4x reduction)

With GQA (8 KV heads instead of 32):
FP16 GQA: 64 GB / 4 = 16 GB
INT4 GQA: 16 GB / 4 = 4 GB (16x total reduction!)
```
</details>

**Q4**: Why does Flash Attention reduce memory usage from O(NÂ²) to O(N)?
<details>
<summary>Answer</summary>
Standard attention materializes the full attention matrix (NÃ—N for sequence length N). Flash Attention uses tiling and online softmax to process attention in blocks, never storing the full matrix. Only tiles (size ~âˆšN) and output (size N) are stored at once, resulting in O(N) memory.
</details>

**Section B: Kernel Optimization**

**Q5**: What is the two-pass algorithm in PagedAttention V1?
<details>
<summary>Answer</summary>
Pass 1: Compute attention scores (QÂ·K), find max (for numerical stability), compute exp and sum for softmax denominator.
Pass 2: Compute output as weighted sum of values using normalized attention weights.
This is necessary for numerically stable softmax computation.
</details>

**Q6**: Explain how PagedAttention V2 differs from V1 for long contexts.
<details>
<summary>Answer</summary>
V1: Single thread block processes entire sequence, limited parallelism for long contexts.
V2: Partitions sequence into chunks, processes in parallel across multiple blocks, then reduces partial results. This provides better parallelism and reduces per-block memory requirements. Typical speedup: 1.5-2x for sequences > 1024 tokens.
</details>

**Q7**: What is kernel fusion and when should you use it?
<details>
<summary>Answer</summary>
Kernel fusion combines multiple operations into a single kernel launch. Benefits: (1) Reduced kernel launch overhead, (2) Reduced memory traffic (intermediate results stay in registers), (3) Better cache locality.

Use when:
- Operations are memory-bound (not compute-bound)
- Intermediate results are large
- Operations are sequential (output of one feeds into next)

Don't use when:
- Would reduce occupancy (too many registers/shared memory)
- Operations can run in parallel
- Increases code complexity significantly
</details>

**Q8**: Compare GPTQ and AWQ quantization methods.
<details>
<summary>Answer</summary>
GPTQ:
- Uses optimal brain quantization (OBQ) with Hessian
- Layer-by-layer reconstruction error minimization
- Group-wise quantization (32-128 weights per group)
- Fast calibration (~128 samples)

AWQ:
- Activation-aware: protects important weights
- Scales channels based on activation magnitude
- Per-channel scaling factors
- Generally better accuracy at 4-bit

Both achieve ~4-bit with <1% perplexity degradation on Llama models.
</details>

---

## ğŸ”¬ Afternoon: Practice & Integration (14:00-18:00)

### Task 4: Comprehensive Quiz - Part 2 (60 min)

**Section C: Advanced Topics**

**Q9**: How does prefix caching work with reference counting?
<details>
<summary>Answer</summary>
```python
# Simplified implementation
class BlockManager:
    def __init__(self):
        self.blocks = {}  # block_id -> Block(data, ref_count)
        self.prefix_cache = {}  # hash(tokens) -> [block_ids]

    def allocate_with_prefix(self, tokens):
        hash_val = hash(tuple(tokens[:prefix_len]))

        if hash_val in self.prefix_cache:
            # Reuse existing blocks
            cached_blocks = self.prefix_cache[hash_val]
            for block_id in cached_blocks:
                self.blocks[block_id].ref_count += 1
            return cached_blocks
        else:
            # Allocate new blocks
            new_blocks = self._allocate_new(tokens)
            self.prefix_cache[hash_val] = new_blocks
            return new_blocks

    def free(self, block_id):
        self.blocks[block_id].ref_count -= 1
        if self.blocks[block_id].ref_count == 0:
            # Actually free the block
            self._return_to_pool(block_id)
```
Blocks are only freed when reference count reaches 0.
</details>

**Q10**: What are tensor cores and how are they used for INT8 GEMM?
<details>
<summary>Answer</summary>
Tensor cores are specialized matrix multiply-accumulate (MMA) units on modern NVIDIA GPUs. For INT8 on A100:
- Operate on 16Ã—16Ã—16 matrix tiles
- Peak: ~624 TOPS (INT8)
- Accessed via WMMA API or PTX instructions

Usage pattern:
```cuda
wmma::fragment<matrix_a, 16, 16, 16, int8_t> a_frag;
wmma::fragment<matrix_b, 16, 16, 16, int8_t> b_frag;
wmma::fragment<accumulator, 16, 16, 16, int32_t> c_frag;

wmma::load_matrix_sync(a_frag, A, K);
wmma::load_matrix_sync(b_frag, B, N);
wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);
```
Provides 10-20x speedup over scalar INT8 operations.
</details>

**Q11**: Explain the online softmax algorithm used in Flash Attention.
<details>
<summary>Answer</summary>
Online softmax allows computing softmax incrementally without seeing all values:

```python
# Update running statistics when seeing new data
def update(m_old, d_old, m_new, d_new):
    m_global = max(m_old, m_new)
    # Correct previous sum for new max
    d_global = d_old * exp(m_old - m_global) + d_new * exp(m_new - m_global)
    return m_global, d_global

# Example: Process tiles
m, d, output = -inf, 0, 0
for tile in tiles:
    m_tile, d_tile, o_tile = process_tile(tile)
    m_new, d_new = update(m, d, m_tile, d_tile)
    # Correct output for updated statistics
    output = output * exp(m - m_new) * (d / d_new) + o_tile
    m, d = m_new, d_new
```
This enables Flash Attention's memory-efficient tiled computation.
</details>

**Q12**: What optimization techniques make reshape_and_cache_kernel efficient?
<details>
<summary>Answer</summary>
1. Vectorized loads/stores: Use uint4 (128-bit) for FP16, loading 8 elements at once
2. Coalesced access: Thread layout ensures consecutive threads access consecutive memory
3. Minimal branching: Avoid divergence within warps
4. Stream parallelism: Multiple streams allow overlapping with computation
5. Async copies: cudaMemcpyAsync for GPUâ†”CPU transfers

Achievable bandwidth: 1500-1650 GB/s on A100 (80-85% of peak).
</details>

**Section D: System Design**

**Q13**: Design a kernel for fused LayerNorm + Linear projection. What optimizations would you use?
<details>
<summary>Answer</summary>
```cuda
__global__ void fused_layernorm_linear_kernel(
    const half* input,      // [M, K]
    const half* weight,     // [K, N]
    const half* ln_weight,  // [K]
    half* output,           // [M, N]
    float eps = 1e-6
) {
    // Each block handles one row
    int row = blockIdx.x;

    // 1. LayerNorm (reduce to shared memory)
    __shared__ float mean, var;
    __shared__ half normalized[K];  // Reuse for multiple columns

    // Compute mean and variance
    // ... (welford algorithm for stability)

    // Normalize and apply weight
    for (int k = threadIdx.x; k < K; k += blockDim.x) {
        float val = (input[row * K + k] - mean) / sqrt(var + eps);
        normalized[k] = val * ln_weight[k];
    }
    __syncthreads();

    // 2. Linear projection (each thread computes output elements)
    for (int n = threadIdx.x; n < N; n += blockDim.x) {
        float sum = 0.0f;
        for (int k = 0; k < K; k++) {
            sum += normalized[k] * weight[k * N + n];
        }
        output[row * N + n] = __float2half(sum);
    }
}
```

Optimizations:
- Single kernel launch (vs 2 separate)
- Normalized values stay in shared memory
- Welford algorithm for numerical stability
- Tiling for large N
- Expected speedup: 1.5-2x
</details>

### Task 5: Integration Project (90 min)

**Project: Build Optimized Inference Pipeline**

Implement a complete optimized inference step combining Week 2 techniques:

```python
#!/usr/bin/env python3
"""
Integration Project: Optimized LLM Inference Step

Combines:
- Quantized weights (INT4 AWQ)
- INT8 KV cache
- Flash Attention
- Fused operations
- Prefix caching
"""

import torch
from typing import List, Tuple

class OptimizedInferenceEngine:
    def __init__(
        self,
        model_name: str,
        quantization: str = "awq",
        kv_cache_dtype: str = "int8",
        use_flash_attn: bool = True,
        enable_prefix_caching: bool = True,
    ):
        self.model_name = model_name
        self.quantization = quantization
        self.kv_cache_dtype = kv_cache_dtype
        self.use_flash_attn = use_flash_attn
        self.enable_prefix_caching = enable_prefix_caching

        # Initialize components
        self._init_model()
        self._init_cache()

    def _init_model(self):
        """Load model with quantization"""
        # TODO: Implement model loading with AWQ/GPTQ
        pass

    def _init_cache(self):
        """Initialize KV cache with chosen dtype"""
        # TODO: Implement cache initialization
        pass

    def prefill(
        self,
        input_ids: torch.Tensor,
        prefix_hash: int = None
    ) -> Tuple[torch.Tensor, dict]:
        """
        Prefill phase with optimizations

        Returns:
            logits: Output logits
            cache_info: Information about cache usage
        """
        batch_size, seq_len = input_ids.shape

        # 1. Check prefix cache
        cached_kv = None
        cache_hit = False
        if self.enable_prefix_caching and prefix_hash is not None:
            cached_kv = self._check_prefix_cache(prefix_hash)
            cache_hit = cached_kv is not None

        # 2. Compute embeddings (could fuse with positional encoding)
        hidden_states = self.embed_tokens(input_ids)

        # 3. Process through layers
        for layer_idx, layer in enumerate(self.layers):
            # a. Pre-attention norm (could fuse with attention)
            normed = self.fused_rmsnorm(hidden_states, layer.ln_weight)

            # b. Attention with optimizations
            if cache_hit and layer_idx < len(cached_kv):
                # Use cached KV for prefix
                attn_out = self._attention_with_cache(
                    normed, cached_kv[layer_idx], layer_idx
                )
            else:
                # Compute and cache
                if self.use_flash_attn:
                    attn_out = self._flash_attention(normed, layer_idx)
                else:
                    attn_out = self._paged_attention_v2(normed, layer_idx)

            # c. Fused residual + norm for MLP
            mlp_input = self.fused_add_norm(
                hidden_states, attn_out, layer.mlp_ln_weight
            )

            # d. MLP (fused matmul + activation)
            mlp_out = self.fused_mlp(mlp_input, layer)

            # e. Residual
            hidden_states = hidden_states + attn_out + mlp_out

        # 4. Final norm + LM head
        logits = self.lm_head(self.final_norm(hidden_states))

        cache_info = {
            "cache_hit": cache_hit,
            "prefix_len": len(cached_kv[0]) if cache_hit else 0,
            "total_len": seq_len,
        }

        return logits, cache_info

    def _flash_attention(
        self,
        hidden_states: torch.Tensor,
        layer_idx: int
    ) -> torch.Tensor:
        """
        Flash Attention implementation with:
        - Online softmax
        - Tiling
        - Fused operations
        """
        # TODO: Implement Flash Attention
        # Use your implementation from Day 9
        pass

    def _paged_attention_v2(
        self,
        hidden_states: torch.Tensor,
        layer_idx: int
    ) -> torch.Tensor:
        """
        PagedAttention V2 with partitioning
        """
        # TODO: Use vLLM's paged attention V2
        pass

    def fused_rmsnorm(
        self,
        x: torch.Tensor,
        weight: torch.Tensor
    ) -> torch.Tensor:
        """
        Fused RMSNorm from Day 13
        """
        # TODO: Use your Triton implementation
        pass

    def fused_mlp(
        self,
        x: torch.Tensor,
        layer
    ) -> torch.Tensor:
        """
        Fused: Quantized GEMM + Activation
        """
        # TODO: Implement fused MLP
        # - Load INT4 weights
        # - Dequantize on-the-fly during GEMM
        # - Apply activation (SiLU/GELU)
        pass

    def benchmark(self):
        """
        Benchmark the full pipeline
        Compare with and without optimizations
        """
        configs = [
            {"name": "Baseline", "opts": {}},
            {"name": "+ Quantization", "opts": {"quantization": "awq"}},
            {"name": "+ INT8 KV", "opts": {"quantization": "awq", "kv_cache_dtype": "int8"}},
            {"name": "+ Flash Attn", "opts": {"quantization": "awq", "kv_cache_dtype": "int8", "use_flash_attn": True}},
            {"name": "+ Prefix Cache", "opts": {"quantization": "awq", "kv_cache_dtype": "int8", "use_flash_attn": True, "enable_prefix_caching": True}},
        ]

        # TODO: Implement benchmarking
        # Measure: latency, throughput, memory usage
        # Expected improvements:
        # - Quantization: 2x memory, 1.5x speed
        # - INT8 KV: 2x memory, 1.1x speed
        # - Flash Attn: 2x speed (long contexts)
        # - Prefix Cache: 4x speed (with hits)
        # Total: ~15-20x improvement!

# Test the engine
if __name__ == "__main__":
    engine = OptimizedInferenceEngine(
        "meta-llama/Llama-2-7b-hf",
        quantization="awq",
        kv_cache_dtype="int8",
        use_flash_attn=True,
        enable_prefix_caching=True,
    )

    # Run benchmark
    engine.benchmark()
```

**Deliverable**: Working implementation demonstrating:
1. Quantized model loading
2. Efficient KV cache management
3. Optimized attention kernels
4. Kernel fusion
5. Performance measurements

### Task 6: Week 3 Preview (60 min)

**What's Coming in Week 3: System Components & Integration**

```
Week 3 Focus Areas:

Days 15-16: Scheduler & Continuous Batching
  - Deep dive into scheduler algorithms
  - Request batching strategies
  - Preemption and swapping policies
  - Performance tuning

Days 17-18: Model Executor & GPU Pipeline
  - Model loading and weight management
  - Execution pipeline orchestration
  - Multi-GPU coordination
  - Worker management

Days 19-21: Distributed Inference
  - Tensor parallelism implementation
  - Pipeline parallelism
  - Communication optimization (NCCL)
  - Scaling to multiple nodes

Key Skills to Develop:
  âœ“ Understanding scheduling algorithms
  âœ“ Debugging distributed systems
  âœ“ Profiling multi-GPU workloads
  âœ“ Optimizing communication patterns
```

**Preparation Tasks**:

1. **Review distributed computing basics**
   - MPI concepts
   - Collective operations (all-reduce, all-gather)
   - NCCL fundamentals

2. **Study vLLM architecture diagrams**
   - How components interact
   - Data flow through system
   - Critical paths

3. **Set up multi-GPU environment** (if available)
   - Test NCCL
   - Run multi-GPU examples
   - Profile communication

---

## ğŸ“ End of Week Summary

### What You Mastered This Week

âœ… **CUDA Fundamentals** (Day 8)
- Memory hierarchy and optimization
- Attention kernel architecture
- Profiling and analysis tools

âœ… **Advanced Kernels** (Day 9)
- PagedAttention V1 and V2
- Flash Attention algorithm
- GQA/MQA optimizations

âœ… **Memory Operations** (Days 10-11)
- Cache reshape and copy kernels
- Swap operations and bandwidth optimization
- Prefix caching and sharing
- Kernel specialization

âœ… **Quantization** (Day 12)
- INT8/INT4/FP8 methods
- GPTQ, AWQ, SmoothQuant
- Fused quantization kernels

âœ… **Custom Operators** (Day 13)
- PyTorch extensions
- Kernel fusion techniques
- Triton programming

### Knowledge Assessment

**Rate your understanding (1-5) of:**

| Topic | Before Week 2 | After Week 2 |
|-------|--------------|--------------|
| CUDA memory hierarchy | ___ | ___ |
| Attention kernel implementation | ___ | ___ |
| Memory optimization techniques | ___ | ___ |
| Quantization methods | ___ | ___ |
| Kernel fusion | ___ | ___ |
| Custom operator development | ___ | ___ |

**Target**: All areas should be 4+ after Week 2

### Key Metrics

**Performance Improvements Learned**:

```
Through Week 2 optimizations, we achieved:

Memory Usage:
  - INT4 quantization: 4x reduction
  - INT8 KV cache: 2x reduction
  - GQA (32â†’8 heads): 4x reduction
  - Total: Up to 32x memory savings!

Speed:
  - Flash Attention: 2-4x (long contexts)
  - Kernel fusion: 1.5-2x
  - Specialized kernels: 1.3-1.8x
  - Quantization: 1.5-2x
  - Prefix caching: 4-8x (with hits)
  - Total: 15-30x end-to-end improvement!

These are real numbers achievable in production!
```

### Weekly Reflection

**Successes**:
- [ ] I can implement CUDA kernels from scratch
- [ ] I understand memory optimization deeply
- [ ] I can profile and debug GPU code
- [ ] I know quantization trade-offs
- [ ] I can create custom PyTorch operators

**Challenges**:
- [ ] Complex template metaprogramming
- [ ] Debugging race conditions
- [ ] Numerical stability issues
- [ ] Performance tuning subtleties
- [ ] Integration complexity

**Carryover Questions for Week 3**:
1. _______________________________________
2. _______________________________________
3. _______________________________________

---

## ğŸš€ Looking Ahead: Week 3 Preparation

**Pre-Week 3 Checklist**:

- [ ] Review all Week 2 notes and code
- [ ] Organize implementations in GitHub repo
- [ ] Set up profiling tools for multi-GPU
- [ ] Read vLLM scheduler code overview
- [ ] Refresh distributed systems concepts
- [ ] Prepare questions for Week 3

**Mindset for Week 3**:
```
Week 2: You mastered the GPU compute layer
Week 3: Time to master the orchestration layer

You'll connect kernels to the full system:
  - How scheduling decisions affect performance
  - How to scale across multiple GPUs
  - How all components work together

This completes your understanding of vLLM!
```

---

## ğŸ“š Additional Resources

**Review Materials**:
- [ ] All daily plans from Days 8-13
- [ ] Module materials in `modules/module_03_cuda_kernels/`
- [ ] Your implementations and notes

**External Resources**:
- [ ] NVIDIA CUDA Best Practices Guide (complete read)
- [ ] FlashAttention paper (re-read with understanding)
- [ ] vLLM blog posts on performance

**Practice Problems**:
- [ ] Implement missing kernels from exercises
- [ ] Optimize existing implementations
- [ ] Create visualization tools for profiling
- [ ] Write performance comparison tools

---

**Week 2 Complete! You're now a CUDA kernel expert! Ready for system-level optimization! ğŸš€**

**Time to advance to Week 3: System Components & Integration**

---

*Week 2 Completed: ___/___/___*
*Total time invested: _____ hours*
*Overall confidence (1-10): _____*
*Readiness for Week 3 (1-10): _____*
