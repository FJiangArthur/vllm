# Day 26: Mock Interviews & Problem Solving Practice

> **Goal**: Practice technical interviews, system design, and problem-solving scenarios
> **Time**: 6-8 hours
> **Prerequisites**: Days 1-25 completed, comprehensive vLLM knowledge
> **Deliverables**: Completed mock interviews, problem solutions, presentation materials

---

## ğŸ“… Daily Schedule

### Morning Session (3-4 hours): System Design Interviews

**9:00-10:00** - System Design Question 1: LLM Serving Platform
**10:00-10:15** - Break & Self-Review
**10:15-11:15** - System Design Question 2: Multi-Tenant GPU Sharing
**11:15-12:30** - System Design Question 3: Optimize for Cost

### Afternoon Session (3-4 hours): Technical Deep Dives

**14:00-15:00** - CUDA Kernel Optimization Problems
**15:00-16:00** - Architecture & Trade-off Questions
**16:00-16:15** - Break
**16:15-17:00** - Performance Debugging Scenarios
**17:00-18:00** - Behavioral & Project Discussion Practice

### Evening (Optional, 1-2 hours): Review & Refinement

**19:00-21:00** - Review answers, identify gaps, refine explanations

---

## ğŸ¯ Learning Objectives

By end of day, you should be able to:
- [ ] Design LLM serving systems from scratch
- [ ] Explain vLLM internals clearly and concisely
- [ ] Solve CUDA optimization problems on whiteboard
- [ ] Debug performance issues systematically
- [ ] Discuss trade-offs with confidence
- [ ] Present your learning journey effectively
- [ ] Handle follow-up questions smoothly

---

## ğŸ“š Morning: System Design Interviews (9:00-12:30)

### Mock Interview 1: Design High-Throughput LLM Serving System (60 min)

**Interviewer Question**:
> "Design a system to serve GPT-4-scale models with maximum throughput. You have a cluster of 100 A100 GPUs. The workload is 10,000 concurrent users with average request rate of 1 request/min per user. Average input: 200 tokens, output: 150 tokens. Optimize for throughput while maintaining <5s P95 latency."

**Your Approach (Think Aloud)**:

```
Step 1: Clarify Requirements (5 min)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Q: What's the priority? Throughput or latency?
A: Throughput is primary, but P95 < 5s is hard requirement.

Q: Can we assume 24/7 operation? Any traffic patterns?
A: Yes 24/7. Peak hours 2x traffic.

Q: Model specifics? Can we use quantization?
A: GPT-4 scale (~220B params). Yes, quantization OK if accuracy acceptable.

Q: Budget constraints?
A: Want maximum throughput per dollar.

Step 2: Back-of-Envelope Calculations (10 min)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Workload:
  - 10,000 users Ã— 1 req/min = 167 requests/sec
  - Average tokens: 200 input + 150 output = 350 tokens/request
  - Total: 167 Ã— 350 = 58,450 tokens/sec needed

Model Size:
  - 220B params Ã— 2 bytes (FP16) = 440 GB
  - Or with INT4: 220B Ã— 0.5 bytes = 110 GB
  - A100 has 80GB â†’ need multi-GPU

GPU Configuration:
  - With INT4: 110GB / 80GB = 2 GPUs minimum (tensor parallel)
  - 100 GPUs / 2 = 50 instances possible
  - Each instance throughput (estimated): 2,000 tokens/sec
  - Total capacity: 50 Ã— 2,000 = 100,000 tokens/sec
  - Headroom: 100,000 / 58,450 = 1.7x (good for peak)

Latency Check:
  - Prefill (200 tokens): ~50ms
  - Decode (150 tokens): 150 Ã— 10ms = 1,500ms
  - Total: ~1,550ms average
  - P95: ~2,000ms (well under 5s âœ“)

Step 3: High-Level Design (15 min)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Load Balancer                        â”‚
â”‚                   (NGINX / HAProxy)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚              â”‚              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚ vLLM Server 1 â”‚ â”‚ vLLM Srv 2â”‚ â”‚ ... Srv 50 â”‚
â”‚  (2x A100)    â”‚ â”‚ (2x A100)  â”‚ â”‚ (2x A100)  â”‚
â”‚  TP = 2       â”‚ â”‚  TP = 2    â”‚ â”‚  TP = 2    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                â”‚              â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Monitoring     â”‚
                â”‚  (Prometheus)   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Components:
  1. Load Balancer: Distribute requests across instances
  2. vLLM Servers: 50 instances, each with 2-GPU TP
  3. Monitoring: Track metrics, auto-scaling

Step 4: Deep Dive - Why vLLM? (10 min)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Framework Choice: vLLM
Reasoning:
  âœ… PagedAttention â†’ maximize batch size
  âœ… Continuous batching â†’ better GPU utilization
  âœ… High throughput focus aligns with requirements
  âœ… Good multi-GPU support (tensor parallelism)
  âœ… Easy deployment and monitoring

Alternative Considered: TensorRT-LLM
  âŒ Lower latency not needed (we have headroom)
  âŒ More complex deployment (50 instances)
  âŒ Higher memory usage â†’ fewer instances possible

Step 5: Optimizations (10 min)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. Quantization: INT4 (AWQ)
   - Reduces memory 4x (440GB â†’ 110GB)
   - Enables 2-GPU deployment
   - 2-3% accuracy loss acceptable

2. PagedAttention Block Size: 16 tokens
   - Balances fragmentation vs overhead
   - 7-8x memory efficiency

3. Batching Configuration:
   - Max batch tokens: 4096 (fit in memory)
   - Continuous batching enabled
   - Mix prefill + decode in same batch

4. Prefix Caching:
   - Cache common system prompts
   - Reduces prefill latency
   - Saves compute

5. Load Balancing Strategy:
   - Round-robin with health checks
   - Route long sequences to less-loaded instances
   - Sticky sessions for better caching

Step 6: Failure Handling (5 min)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

GPU Failure:
  - Health checks detect failure
  - Load balancer removes instance
  - Auto-scale launches replacement
  - Redundancy: 1.7x capacity headroom

High Load (Peak Hours):
  - Monitor queue depth
  - If queue > threshold, scale up
  - Add instances dynamically
  - Warm-up time: ~2 minutes

Request Timeout:
  - Client timeout: 30s
  - Server timeout: 25s (fails before client)
  - Retry with exponential backoff

Step 7: Monitoring & Metrics (5 min)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Key Metrics:
  - Throughput: tokens/sec per instance
  - Latency: P50, P95, P99
  - GPU utilization: Should be >85%
  - Queue depth: Indicator for scaling
  - Error rate: Should be <0.1%
  - Memory usage: Block allocation efficiency

Alerts:
  - P95 latency > 4s â†’ Warning
  - GPU utilization < 70% â†’ Underutilization
  - Error rate > 1% â†’ Critical
  - Queue depth > 100 â†’ Scale up

Follow-up Questions You Might Get:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Q: "What if we need to support streaming?"
A: "vLLM supports streaming natively. We'd modify the API to
   return tokens as they're generated. This actually improves
   perceived latency. No architecture change needed."

Q: "How would you handle model updates?"
A: "Blue-green deployment:
   1. Deploy new model to inactive pool
   2. Run validation tests
   3. Gradually shift traffic (10%, 25%, 50%, 100%)
   4. Monitor metrics at each step
   5. Rollback if issues detected"

Q: "What if latency requirements become stricter (<1s P95)?"
A: "Would need to reconsider:
   1. Switch to TensorRT-LLM for lower latency
   2. Use more aggressive quantization (INT4 â†’ FP8)
   3. Reduce batch size (latency vs throughput trade-off)
   4. Add more GPUs to reduce queueing
   5. Consider speculative decoding"

Q: "How do you optimize cost?"
A: "Current cost analysis:
   - 100 GPUs Ã— $2/hour = $200/hour
   - Throughput: 100k tokens/sec
   - Cost: $0.0072 per 1M tokens

   Optimizations:
   1. Right-size instances (maybe 4-GPU vs 2-GPU)
   2. Spot instances for non-critical load
   3. Better request packing (prefix caching)
   4. Schedule maintenance during low traffic"
```

**Self-Evaluation Rubric**:

```
â–¡ Clarified requirements (5 points)
â–¡ Performed calculations (10 points)
â–¡ Designed architecture diagram (15 points)
â–¡ Justified technology choices (20 points)
â–¡ Discussed optimizations (20 points)
â–¡ Considered failure modes (15 points)
â–¡ Defined monitoring (10 points)
â–¡ Handled follow-ups (5 points)

Total: ___/100
Target: 80+
```

### Mock Interview 2: Multi-Tenant GPU Sharing (60 min)

**Interviewer Question**:
> "Design a system where multiple customers share the same GPU cluster for LLM inference. Each customer has different SLAs, priorities, and budgets. How do you ensure fairness, isolation, and meet SLAs?"

**Your Answer Framework**:

```
Step 1: Understand Requirements
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Customer Tiers:
  - Premium: P95 latency <500ms, guaranteed capacity
  - Standard: P95 latency <2s, best effort
  - Budget: P95 latency <5s, preemptible

Isolation Needs:
  - Data isolation (critical)
  - Performance isolation (important)
  - Cost allocation (required)

Step 2: Architecture
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Request Router                    â”‚
â”‚    - Authenticate                         â”‚
â”‚    - Classify tier                        â”‚
â”‚    - Route to queue                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”
â”‚Premiumâ”‚ â”‚Standardâ”‚ Budgetâ”‚
â”‚ Queue â”‚ â”‚ Queue  â”‚ Queue â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜
    â”‚        â”‚         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Smart Scheduler â”‚
    â”‚ - Priority      â”‚
    â”‚ - Fair sharing  â”‚
    â”‚ - Preemption    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  GPU Cluster    â”‚
    â”‚  (vLLM servers) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 3: Scheduling Algorithm
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

class MultiTenantScheduler:
    def schedule_next_batch(self):
        """Select requests for next batch"""

        batch = []
        batch_tokens = 0
        max_tokens = 4096

        # 1. Always schedule premium (guaranteed)
        while self.premium_queue and batch_tokens < max_tokens * 0.6:
            req = self.premium_queue.pop()
            batch.append(req)
            batch_tokens += req.tokens

        # 2. Fill with standard (fair share)
        while self.standard_queue and batch_tokens < max_tokens * 0.9:
            req = self.standard_queue.pop()
            batch.append(req)
            batch_tokens += req.tokens

        # 3. Fill remaining with budget (best effort)
        while self.budget_queue and batch_tokens < max_tokens:
            req = self.budget_queue.pop()
            batch.append(req)
            batch_tokens += req.tokens

        return batch

    def handle_preemption(self):
        """Preempt budget requests if premium SLA at risk"""

        if self.premium_queue_depth > threshold:
            # Preempt running budget requests
            for req in self.running_requests:
                if req.tier == 'budget':
                    self.preempt(req)  # Save state, free GPU
                    self.budget_queue.append(req)

Step 4: Isolation Mechanisms
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Data Isolation:
  âœ… Separate KV cache blocks per customer
  âœ… Clear memory between requests
  âœ… No prefix sharing across customers

Performance Isolation:
  âœ… Priority-based scheduling
  âœ… Guaranteed capacity allocation
  âœ… Budget requests preemptible

Cost Isolation:
  âœ… Track GPU time per customer
  âœ… Token-level metering
  âœ… Separate billing per tier

Step 5: SLA Enforcement
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Premium Tier:
  - Reserved capacity: 40% of cluster
  - Queue priority: Highest
  - Preemption: Never
  - If SLA at risk: Preempt budget tier

Standard Tier:
  - Fair share: 40% of cluster
  - Queue priority: Medium
  - Preemption: Only by premium

Budget Tier:
  - Best effort: Remaining capacity
  - Queue priority: Low
  - Preemption: By premium or standard
```

---

## ğŸ’» Afternoon: Technical Deep Dives (14:00-18:00)

### CUDA Kernel Optimization Problem (60 min)

**Interviewer Question**:
> "Here's a simple attention kernel. It's running slow. Identify bottlenecks and optimize it. Explain your reasoning."

```cuda
// Given: Slow attention kernel
__global__ void slow_attention(
    float* output,
    const float* query,
    const float* key,
    const float* value,
    int seq_len,
    int head_dim
) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;

    if (tid < seq_len) {
        // Compute attention for this token
        float max_score = -INFINITY;

        // Pass 1: Compute scores and find max
        for (int i = 0; i < seq_len; i++) {
            float score = 0.0f;
            for (int d = 0; d < head_dim; d++) {
                score += query[tid * head_dim + d] * key[i * head_dim + d];
            }
            if (score > max_score) {
                max_score = score;
            }
        }

        // Pass 2: Compute exp and sum
        float sum = 0.0f;
        for (int i = 0; i < seq_len; i++) {
            float score = 0.0f;
            for (int d = 0; d < head_dim; d++) {
                score += query[tid * head_dim + d] * key[i * head_dim + d];
            }
            sum += expf(score - max_score);
        }

        // Pass 3: Compute weighted output
        for (int d = 0; d < head_dim; d++) {
            float out = 0.0f;
            for (int i = 0; i < seq_len; i++) {
                float score = 0.0f;
                for (int j = 0; j < head_dim; j++) {
                    score += query[tid * head_dim + j] * key[i * head_dim + j];
                }
                float weight = expf(score - max_score) / sum;
                out += weight * value[i * head_dim + d];
            }
            output[tid * head_dim + d] = out;
        }
    }
}
```

**Your Optimization Process**:

```
Step 1: Identify Problems (Whiteboard Analysis)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Problem 1: Redundant Computation
  - Computing QÂ·K three times! (passes 1, 2, 3)
  - For seq_len=1024, head_dim=128:
    - Ops per token: 3 Ã— 1024 Ã— 128 = 393,216 ops
    - Should be: 1024 Ã— 128 = 131,072 ops
  - Wasting 3x compute!

Problem 2: Uncoalesced Memory Access
  - query[tid * head_dim + d]: Strided access
  - Different threads access far-apart memory
  - ~32x bandwidth loss

Problem 3: No Shared Memory Usage
  - Re-reading query from global memory repeatedly
  - Slow global memory access every time

Problem 4: Low Parallelism
  - Only seq_len threads launched
  - For seq_len=1024: Only 1024 threads
  - A100 can run 2048 threads per SM Ã— 108 SMs!

Step 2: Optimized Version
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

__global__ void optimized_attention(
    float* output,
    const float* query,
    const float* key,
    const float* value,
    int seq_len,
    int head_dim
) {
    // Each block handles one output token
    int token_idx = blockIdx.x;
    int tid = threadIdx.x;

    // Shared memory for query and intermediate results
    __shared__ float s_query[128];  // Assuming head_dim <= 128
    __shared__ float s_scores[1024]; // Assuming seq_len <= 1024
    __shared__ float s_max;
    __shared__ float s_sum;

    // Load query into shared memory (coalesced)
    if (tid < head_dim) {
        s_query[tid] = query[token_idx * head_dim + tid];
    }
    __syncthreads();

    // Compute scores (parallelized across threads)
    float local_max = -INFINITY;

    for (int i = tid; i < seq_len; i += blockDim.x) {
        float score = 0.0f;
        // Use shared memory query (fast)
        for (int d = 0; d < head_dim; d++) {
            score += s_query[d] * key[i * head_dim + d];
        }
        s_scores[i] = score;  // Store once
        local_max = fmaxf(local_max, score);
    }

    // Reduce to find global max
    local_max = block_reduce_max(local_max);
    if (tid == 0) s_max = local_max;
    __syncthreads();

    // Compute softmax denominator
    float local_sum = 0.0f;
    for (int i = tid; i < seq_len; i += blockDim.x) {
        float exp_score = expf(s_scores[i] - s_max);
        s_scores[i] = exp_score;  // Overwrite with exp
        local_sum += exp_score;
    }

    local_sum = block_reduce_sum(local_sum);
    if (tid == 0) s_sum = local_sum;
    __syncthreads();

    // Compute output (each thread handles some dimensions)
    for (int d = tid; d < head_dim; d += blockDim.x) {
        float out = 0.0f;
        for (int i = 0; i < seq_len; i++) {
            float weight = s_scores[i] / s_sum;
            out += weight * value[i * head_dim + d];
        }
        output[token_idx * head_dim + d] = out;
    }
}

Step 3: Performance Improvement Analysis
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Original:
  - Compute: 3 Ã— seq_len Ã— head_dim ops per token
  - Memory: seq_len Ã— head_dim global reads Ã— 3 passes
  - Time (estimated): 500ms for seq=1024

Optimized:
  - Compute: 1 Ã— seq_len Ã— head_dim ops per token
  - Memory: seq_len Ã— head_dim global reads Ã— 1 pass
  - Time (estimated): 80ms for seq=1024

Speedup: 500ms / 80ms = 6.25x

Improvements:
  âœ… Eliminate redundant computation: 3x
  âœ… Use shared memory: 1.5x
  âœ… Better parallelism: 1.3x
  âœ… Total: ~6x speedup

Step 4: Further Optimizations (Advanced)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Could further improve with:
  1. Warp-level primitives (__shfl_down)
  2. Vectorized loads (float4)
  3. Tiling for large seq_len
  4. Flash Attention algorithm (block-sparse)
```

**Interviewer Follow-up**: "Walk me through your thought process for finding the bottleneck."

**Your Answer**:
```
My systematic approach:

1. Understand the algorithm
   - Read code carefully
   - Identify what it's computing
   - Understand expected complexity

2. Profile mentally or actually
   - Count operations
   - Identify memory accesses
   - Look for patterns

3. Apply knowledge
   - CUDA best practices checklist:
     â–¡ Coalesced memory?
     â–¡ Shared memory used?
     â–¡ Enough parallelism?
     â–¡ Redundant work?

4. Prioritize fixes
   - Biggest bottleneck first
   - Low-hanging fruit
   - Measure impact

In this case:
  - Redundant computation was obvious (3x same work)
  - That's the first thing to fix
  - Then memory and parallelism
```

### Architecture & Trade-off Questions (60 min)

**Question 1**: "Explain PagedAttention to a senior engineer."

**Your Answer** (2-3 minutes):
```
"PagedAttention is vLLM's key innovation for memory-efficient LLM serving.
It's inspired by virtual memory in operating systems.

The Problem:
Traditional KV cache requires contiguous memory allocation. If you
allocate for max sequence length (say 2048 tokens), but most sequences
are much shorter (maybe 200 tokens), you waste 90% of memory. This
limits batch size and throughput.

The Solution:
PagedAttention divides KV cache into fixed-size blocks (like OS pages).
A block table maps logical block indices to physical block indices,
similar to a page table. Blocks don't need to be contiguous.

Example:
  Sequence A (42 tokens, needs 3 blocks):
    Logical blocks: [0, 1, 2]
    Physical blocks: [5, 2, 8]  â† Can be anywhere!

Benefits:
1. No fragmentation - free blocks can go anywhere
2. On-demand allocation - only allocate as sequence grows
3. Memory sharing - multiple sequences can share same blocks
   (useful for beam search, prefix caching)

Performance:
  - 7-8x memory efficiency improvement
  - Enables much larger batch sizes
  - ~10% latency overhead from indirection (acceptable trade-off)

Implementation:
  - CUDA kernel uses block table for indirection
  - Block manager handles allocation/deallocation
  - Python scheduler coordinates everything

This is why vLLM achieves higher throughput than alternatives -
it can pack more concurrent requests into the same GPU memory."
```

**Question 2**: "Compare continuous batching vs static batching."

**Your Answer**:
```
Static Batching (Traditional):
  - Batch requests together
  - Process until ALL finish
  - Then start new batch

  Example timeline:
    Batch 1: [A, B, C, D] â†’ Start
    t=1s: D finishes, but must wait for A, B, C
    t=2s: C finishes, must wait for A, B
    t=3s: B finishes, must wait for A
    t=4s: A finishes â†’ Batch 1 done
    t=4s: Batch 2 starts

  Problems:
    - GPU idle while waiting for slowest request
    - High latency for new requests (must wait for batch)
    - Wasted compute

Continuous Batching (vLLM):
  - Dynamically add/remove requests every iteration
  - No waiting for entire batch to finish

  Example timeline:
    t=0s: Running: [A, B, C, D]
    t=1s: D finishes â†’ Add E â†’ Running: [A, B, C, E]
    t=2s: C finishes â†’ Add F â†’ Running: [A, B, E, F]
    ...

  Benefits:
    - Better GPU utilization (always full batch)
    - Lower average latency (new requests start faster)
    - 2-3x throughput improvement

  Trade-offs:
    - More complex scheduling
    - Per-iteration overhead (~10-20Î¼s)
    - Need efficient memory management (PagedAttention!)

In production:
  - Continuous batching is almost always better
  - Only use static if: simple workload, all same length
  - vLLM's implementation is very efficient
```

**Question 3**: "When would you choose TensorRT-LLM over vLLM?"

**Your Answer**:
```
Choose TensorRT-LLM when:

1. Latency is Critical
   - Real-time applications (<100ms SLA)
   - Interactive chatbots
   - Live translation

   Why: TensorRT-LLM has 20-40% lower latency through:
   - Fully optimized kernels
   - No block table indirection
   - CUDA graphs for launch overhead
   - Better cache locality

2. You Have Predictable Workloads
   - Known sequence lengths
   - Consistent batch sizes
   - Limited concurrency

   Why: Can pre-allocate memory, less fragmentation concern

3. Need Advanced Quantization
   - FP8 on H100 (best accuracy/speed)
   - Custom quantization schemes
   - Per-tensor quantization

   Why: TensorRT-LLM has most advanced quant support

4. NVIDIA Ecosystem
   - Already using Triton server
   - Need enterprise support
   - Integration with other NVIDIA tools

   Why: First-class NVIDIA integration

Choose vLLM when:

1. Maximize Throughput
   - Cloud serving
   - High concurrency (100+ requests)
   - Variable sequence lengths

   Why: PagedAttention enables 30-50% higher throughput

2. Memory Constrained
   - Limited GPU memory
   - Need to pack many requests

   Why: 7-8x better memory efficiency

3. Need Flexibility
   - Research and experimentation
   - Custom modifications
   - Rapid iteration

   Why: Python-based, easy to hack

4. Cost Optimization
   - Want maximum throughput per $
   - Minimize number of GPUs

   Why: Better utilization = lower cost

My recommendation matrix:
  - Latency SLA <100ms â†’ TensorRT-LLM
  - Latency SLA 100ms-1s â†’ Either (depends on other factors)
  - Latency SLA >1s â†’ vLLM
  - Throughput priority â†’ vLLM
  - Easy deployment â†’ HuggingFace TGI
```

---

## ğŸ“ End of Day Summary

### Mock Interview Performance Review

**System Design (1-3)**:
- [ ] Clarified requirements thoroughly
- [ ] Performed accurate calculations
- [ ] Drew clear architecture diagrams
- [ ] Justified technology choices
- [ ] Discussed trade-offs explicitly
- [ ] Considered failure modes
- [ ] Defined monitoring strategy

**Technical Deep Dives**:
- [ ] Explained complex concepts clearly
- [ ] Used concrete examples
- [ ] Discussed trade-offs
- [ ] Showed systematic thinking
- [ ] Handled follow-up questions

**Problem Solving**:
- [ ] Identified bottlenecks correctly
- [ ] Proposed effective solutions
- [ ] Explained reasoning clearly
- [ ] Considered alternatives

### Areas for Improvement

**Identify your weak spots**:
```
Weak Area 1: _______________________
Action: ____________________________

Weak Area 2: _______________________
Action: ____________________________

Weak Area 3: _______________________
Action: ____________________________
```

### Key Talking Points to Remember

```
1. PagedAttention: "7-8x memory efficiency through block-based allocation"

2. Continuous Batching: "Dynamic batching â†’ 2-3x throughput improvement"

3. vLLM vs TRT-LLM: "Throughput vs Latency trade-off"

4. CUDA Optimization: "1. Coalescing, 2. Shared mem, 3. Reduce redundancy"

5. Production: "Monitoring, scaling, failure handling"
```

---

## ğŸš€ Preview: Day 27

Tomorrow's focus:
- **More Problem Solving**: Algorithm questions, debugging scenarios
- **Code Review Practice**: Analyzing kernel code
- **Technical Writing**: Documenting solutions
- **Presentation Skills**: Explaining to different audiences

**Preparation**:
- Review today's weak areas
- Practice explanations out loud
- Prepare more examples
- Get feedback if possible

---

**Completed: ___/___/___**
**Time spent: _____ hours**
**Self-assessment: ___/10**
**Areas to improve: ________________**
